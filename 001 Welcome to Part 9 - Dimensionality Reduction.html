<p>Welcome to Part 9- Dimensionality Reduction!</p>

<p><br></p>











<p>Remember in Part 3 - Classification, we worked with datasets composed of only two independent variables. We did for two reasons:</p>







<ol><li>Becausewe needed two dimensionsto visualize better how Machine Learning models worked (by plotting the prediction regions and the prediction boundary for each model).</li><li>Because whatever is the originalnumber of ourindependent variables, we can often end up with two independent variables by applying anappropriate Dimensionality Reduction technique.</li></ol>



<p><br></p>



<p>There are two types of Dimensionality Reduction techniques:<br></p>







<ol><li>Feature Selection</li><li>Feature Extraction</li></ol>







<p><br></p>



<p>Feature Selection techniques are Backward Elimination, Forward Selection, Bidirectional Elimination,Score Comparison and more. We covered these techniques in Part 2 - Regression.</p>







<p>In this part we will cover the followingFeature Extraction techniques:</p>







<ol><li>Principal Component Analysis (PCA)</li><li>Linear Discriminant Analysis (LDA)</li><li>Kernel PCA</li><li>Quadratic Discriminant Analysis (QDA)</li></ol>







<p><br></p>







<p>Enjoy Machine Learning!<br></p>