<p>Meet CatBoost  the gradient-based model that is bound to beat out the power that LightGBM (a gradient-based model that uses tree-based learning algorithms) has to offer on specific problems.</p><p>CatBoost is a great self-tuning model to have in the toolkit whenever you want to get the highest accuracy on datasets that have many categorical features, which is usually the case with on-the-job problems.</p><p>In this bonus, you will explore CatBoost theory and implement CatBoost on the brain cancer dataset (the same dataset we used for XGBoost).</p><p>This tutorial covers the following steps:</p><ul><li><p>Importing the libraries</p></li><li><p>Importing the dataset</p></li><li><p>Splitting the dataset into the Training and Test set</p></li><li><p>Training XGBoost on the Training set</p></li><li><p>Making the confusion matrix</p></li><li><p>Applying k-fold Cross-Validation</p></li></ul><p><a href="https://www.superdatascience.com/pages/ml-model-selection-bonus" rel="noopener noreferrer" target="_blank"><strong>Click here to access</strong></a> all the files youll need to implement this project yourself and view the tutorial.</p><p><br></p>